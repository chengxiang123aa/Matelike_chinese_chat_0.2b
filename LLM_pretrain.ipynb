{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f00933-f00a-422e-aaa4-dc1b6a80aeff",
   "metadata": {},
   "source": [
    "- Step 1.导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f663d8f8-3a99-4f19-a0c6-a2dde386e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import psutil\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8f30c-d2fd-41cb-aea1-8addb7279fdf",
   "metadata": {},
   "source": [
    "- Step 2.定义BOS和EOS标记，并加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff895d52-6ce9-4026-b30d-6432a79c8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义BOS和EOS标记\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "132613a9-e31d-493b-9bed-e70f7cf38f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的tokenizer词表大小: 6400\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的分词器路径\n",
    "tokenizer = AutoTokenizer.from_pretrained('MateConv/model/mateconv_tokenizer', use_fast=False)\n",
    "print(f'加载的tokenizer词表大小: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed63f5-2774-4e49-ac14-53362b7d3a41",
   "metadata": {},
   "source": [
    "- Step 3.读取部分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5e5a48-e767-422a-81a0-29d973c4352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 行数据: {'text': '在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。在计算税款损失时，要不要将进项留抵税额包括在内？\\n对此，实务中存在意见分歧。\\n有人主张归并，即计算税款损失时包括进项留抵税额；\\n有人主张剥离，即计算税款损失时剔除进项留抵税额。分析这个问题，需要确定进项留抵税额与税款损失之间是什么关系。\\n理清这二者之间的关系，首先需要了解增值税的概念和其抵扣机制。增值税是以商品（货物、服务等）在流转过程中产生的增值额作为计税依据而征收的一种流转税。为避免重复征税，在增值税中存在抵扣链条机制。\\n一般而言，交易上游企业缴纳的税额，交易下游企业可以对相应的税额进行抵扣。\\n对增值税一般纳税人来说，其购进货物、服务等取得增值税专用发票，发票上的税额是进项税额。\\n其出售货物、服务等，向购买方开具增值税专用发票，发票的税额是销项税额。\\n一般情况下，销项税额减去进项税额的金额是应纳税额，企业根据应纳税额按期申报纳税。\\n其次需要了解进项留抵税额的概念及产生原因。\\n在计算销项税额和进项税额的差额时，有时会出现负数，即当期进项税额大于当期销项税额。这个差额在当期未实现抵扣，为进项留抵税额，在以后纳税人有销项税额时再进行抵扣。\\n企业产生进项留抵税额的主要原因是其进项税额和销项税额时间上的不一致。\\n例如，企业前期集中采购货物和服务，投资大，销项税率低于进项税率等。\\n从税款抵扣的角度看，进项留抵税额只是购进的这部分进项税额参与到增值税应纳税额的计算过程中，但是其对应的进项税额抵扣还未真正实现，一般要等到其未来有相应的销项税额时，才能真正实现进项税额抵扣。\\n可见，进项留抵税额处于不确定状态，能否抵扣受到很多因素影响，例如企业经营中断，没有销项税额，这时进项留抵税额就无法实现抵扣。但如果企业按照税收政策规定申请进项留抵退税，进项税额抵扣就随之实现。\\n最后需要了解税款损失的概念。\\n税款损失，通常是指因虚开增值税专用发票，导致国家税款被骗或者流失的金额。关于税款损失，实务中有多种表述。\\n例如，北京大学法学院教授陈兴良曾谈到虚开行为本身不会造成国家税款损失，只有利用发票抵扣时才会造成国家税款损失。刘兵等编著的《虚开增值税专用发票案例司法观点和案例解析》一书中提到：“给国家税款造成损失的数额，实际上就是被骗取的国家税款在侦查终结以前无法追回的部分。”\\n赵清海与王家欣合著的《增值税专用发票虚开的判定与预防》一书中提到：“司法实践中，受票方用虚开的增值税专用发票予以抵扣的税款，从而导致受票方应纳税额的减少是法院所认定的国家税款流失的金额。”\\n从这些表述可见，税款损失应该是实际造成的损失，不应包括不确定的部分——进项留抵税额，进项留抵税额与税款损失之间不能直接画等号。\\n综上分析，进项留抵税额，只是使国家税款处于可能被抵扣的状态，还没有真正造成国家税款流失，一般情况下应将其从税款损失中剥离，特殊条件下将其归并入税款损失。\\n例如，当纳税人造假按照税收政策规定申请进项留抵税额退税后，有关税款损失将会从危险状态转化成危害结果，这时候要将有关进项留抵税额并入税款损失。\\n所以，在虚开增值税专用发票案件中，一般情况下，如果以纳税人的进项税额作为税款损失的计算基数，在对其进行行政处罚或刑事处罚时，应把进项留抵税额从税款损失中剔除，但纳税人申请进项留抵退税的除外。这样处理，把处罚与危害结果相对应，体现行政处罚法的过罚相当原则和刑法的罚当其罪原则。'}\n",
      "第 2 行数据: {'text': '读者在使用本《年鉴》时发现与以前本局出版、公布、或内部提供的资料有出入的，概以本《年鉴》为准。\\n《年鉴》正文内容分为三大部分。第一部分为文字部分，收录了《2012年政府工作报告》以及《2011年河源市国民经济和社会发展统计公报》。第二部分为统计图，形象地反映建市以来河源市国民经济发展变化情况。第三部分为统计资料，具体分为行政区划和自然资源，综合、核算、人口，农村经济，工业，能源，交通、邮电，贸易业、物价指数，对外经济、旅游，财政、金融和保险，固定资产投资与建筑业，劳动工资，人民生活，文教、卫生和其他，河源市乡镇主要经济指标，广东省县域主要经济指标,广东省各市主要经济指标等16部分。此外，为便于读者正确理解和使用统计资料，特附主要统计指标解释、统计术语简介及统计法律法规等资料。\\n《年鉴》中，本市的数据是根据我局及有关部门的统计年报整理汇编而成，由于某些专业统计制度和统计口径的变化，有些数据空缺。使用本《年鉴》时，请注意指标名称的含义、统计口径、统计范围、计算单位、可比价与现行价(当年价)等。\\n《年鉴》第一部分中的有些数据为初步统计数，凡与本《年鉴》中第三部分的数据有出入的，则以第三部分的统计数据为准。\\n本《年鉴》部分统计数据使用了四舍五入的进位方法，因此，可能令统计表内个别项目相加与总数略有出入。\\n本《年鉴》统计表中符号使用说明：“＃”表示其中主要项；“空格”表示该项统计指标数据不详或无该项数据。\\n本《年鉴》的编辑出版，得到县、区及市直有关部门和单位的大力支持，在此表示感谢！本书疏漏之处敬请批评指正。\\n下载说明： �本站下载的文件一律为压缩文件，请使用 WinRAR 解压。\\n�PDF格式的资料请使用 Adobe Reader 浏览。\\n�本站提供的一些资料是供学习研究之用，如用于商业用途，请购买正版。'}\n",
      "第 3 行数据: {'text': '初中阶段是学生身心发育的一个突变期。尤其是初一学生，从小学到中学，随着环境改变，课程增多，难度加大，他们内心发生了急剧变化，产生了许多烦恼、困惑，造成较大的心理偏差，这就需要教师和家长及时给予心理指导和帮助。\\n一、心理偏差的种种表现\\n1、骄傲自负心理。这种心理偏差主要表现在思维敏捷、小学成绩拔尖的学生身上，特别是一些长期担任班干部、竞赛获奖、父母有权力的学生表现尤为明显。\\n2．单纯求趣心理。求趣激趣，这是教学的原则之一，但是，有些初一学生过分地追求接受知识要符合自己的兴趣，还想回到幼儿园、小学时“游戏教育”和“愉快教育”中去，不能努力适应初中阶段的学习生活。\\n3．自卑孤僻心理。多数来自普通工薪家庭及农村贫困地区或遭遇父母婚变的学生，往往在干部、富家子弟、有特长的同学面前感到自卑，心理压抑，行为孤僻，甚至变态的自尊，影响学习。\\n4．胆怯畏惧心理。部分性格内向、胆小的学生，主要是女生，羞于用语言表达思想，沉溺于内心活动和笔头表达。内心活动不能外显，妨碍了思维素质的深入发展。\\n5．浮躁马虎心理。部分活泼好动的学生，智力水平不低，但就是不能静下心学习，总是浅尝辄止，马虎应付，不愿作深入的思考，常常“半罐水响叮当”。\\n6．贪图享受心理。一些家境较好的学生，行为懒散，好逸恶劳，学习上畏难怕苦，生活上讲吃讲穿。\\n二、上述心理偏差的形成原因\\n1．生理上的原因。初中学生处于发育高峰期，身高体重剧增，性发育开始。生理上的急剧变化使儿童意识到自己不再是孩子，“成人感”增强。但是，青年身体成熟速度存在着很大的个体差异：不同性别之间相差两年左右，同性别之间相差四年左右。因此，同是初中学生，一部分学生生理已跨入青年期，而另一部分学生可能还停留在童年期。\\n2．心理上的原因。随着生理的变化，“成人感”的出现，初中学生心理产生“独立”，力求摆脱对成人的依赖，老师、家长在他们心目中的权威降低，同学之间相互影响增强。思维上发展了批判性，但由于经验的缺乏含有片面性和主观性；行为上出现“独特性”和“受暗示性”乃至“抗拒性”，即逆反心理；情绪上带有冲动性，不善于克制自己；兴趣和愿望上带有随意性、多变性、狂热性，常为了所谓讲“义气”而庇护同伴，或为同伴打抱不平；感情上具有“闭锁性”，而对于艰苦的学习活动特别重要的意志品质，则还处在比较软弱的状态。\\n3．环境的原因。心理学认为，个体的生物遗传因素规定了发展的潜在可能范围，而个体环境教育则确定他在此可能范围内的现实水平。环境条件有利与否对个体发展的现实水平起了决定性作用。\\n①家庭。社会的信仰、观念等社会化目标都是首先通过父母的过渡，以高度个性化了的、有选择的形式传递给儿童的。父母本身的个性特征、社会地位、教育水平、宗教信仰、价值标准等等都强烈地影响他们的后代。父母的教养方式、家庭结构、物质条件、人际环境、文化和情绪氛围，都在很大程度上影响着学生。\\n②学校。学校不仅是对学生传授文化科学知识，进行政治思想教育的社会基本教育单元，还是促进学生良好品格形成和发展的重要场所。学生在学校里形成良好的品格，才能顺利走向社会，适应社会生活。反之，则会发生各种问题。而现在的应试教育制度，像紧箍咒一样，时时冲击着素质教育，教师以升学率论质量给待遇，使一些教师对成绩好的学生倍加宠爱，对成绩差的学生则百般呵斥。更有少数教师将腐朽庸俗的人际关系引入师生和家长的关系，身教言传，污染了学生心灵；让孩子过早成人化、世故化。\\n③社会。社会上各种腐朽思想沉渣泛起，对学生负面影响很大。影视传媒、流失少年、勒索等等，浸染着学生稚嫩的心灵；电子游戏机、卡拉OK厅等，又使我们的孩子面临着极强的诱惑，意志薄弱者稍不留意，便坠入其间。\\n三、纠正初中学生的心理偏差的对策\\n为了纠正初中学生的心理偏差，我们必须对教育环境影响予以高度重视。在现有环境中，我们应做到：\\n1．坚持以德、智、体、美、劳全面的教育方针为指导思想进行教育管理，坚持“要成才先成人”的教育思想。\\n2．“学高为师，身正为范。”作为教师，必须加强道德修养，提高职业素质，全面关心和爱护每一位学生的身心健康发展。\\n3．以激励为心育的主要手段。我们要将思想教育和学生喜闻乐见的实践活动结合起来，不断提高学生对美的感受和鉴赏力，使其求真向善，茁壮成长。\\n4．形成教育合力。在抓好班集体建设的同时，我们必须密切联系家长，与家长一起研究分析学生，共同教育学生。\\n5．帮助学生正确认识、分析、评价自己的心理过程。让他们将社会化标准－－《中学生日常行为规范》逐渐内化，用以规范自己的言行，自觉抵制不良诱惑，不断提高自我意识水平和自我教育能力。\\n6．对各类心理偏差学生施以不同的教育。对有骄傲自负心理的学生施以“挫折教育”；对有自卑、胆怯畏惧心理的学生施以“磨难教育”；对有虚荣忌妒、趋同报复、庸俗心理的学生施以分辨真美善、假丑恶的“是非教育”等。\\n与此同时，还应努力提高、优化当代中学生的心理特点。\\n首先，作为家长必须转变观念。对自己的孩子，在作业和职业方面的“期望值”不能脱离子女的实际而好高骛远，每个孩子因智力因素、情趣爱好，性格意志和心理承力各不相同，如果孩子确实尽了自己的努力，而未达到你所期望的目标，不应过多责怪，更不能冷嘲热讽，惩罚打骂。诚然，家长望子成龙“天经地义”，无可厚非。但“龙”的内涵并不专指读大学、考研究生。“三百六十行，行行出状元”，如果每一位家长都能建立这样的“职业观”，让孩子在宽松的环境里读书，\\n其次，作为教育者----教师来说，则更要不断学习，及时吸收新鲜气息，不断提高自己的思想、政治教育水平，提高自己的专业知识和业务水平，做到不仅能教书育人，更能进行教育评价，尊重学生人格，依法执教，用先进的具有创造性的教育思想、理论、方法促进教育水平的提高，注重培养学生的全面发展，加强能力培养和思维训练，提高学生的综合素质。具体方法如下：\\n第一，让学生充分了解自己的心理特点，通过与周围的同学以及其他同龄人相比，通过同电影、小说电视里特定情景中的人物相比，如宣传奥斯特洛夫斯基、托尔斯泰、张海迪、贝多芬等等，通过对比，找出自己在哪些方面存在弱点，或者也可以通过父母、老师、同学对自己经常的评价了解自己在哪些方面存在不良心理特点，从而扬长避短。\\n第二，选择恰当的方法进行锻炼。例如：\\n1、教他们多读好书，如《周恩来》、《钢铁是怎样炼成的》等优化心理品质。人类的几千年文明，其智慧、经验、真知灼见，都浓缩于书中，如果多读好书，能经常与这样一些“高尚朋友”对话，听听他们的“指点”以此开阔视野，启迪智慧，这对优化学生的心理品质是大有裨益的，作为中学生，不仅要读好的故事书，还应该读一些伟人的传记，读一些思想、修养方面的书籍，并且养成做读书笔记的习惯。\\n2、鼓励学生参加社会活动，锻炼心理品质，如送“温暖小组”、“助残小分队”等活动的开展，都是锻炼心理品质行之有效的方法。\\n3、也要注重培养学生琴、棋、书、画、音、体、美等美育活动，有助于疏导、排解不良情绪，给人以美的熏陶和享受，从而对心理产生良性刺激。让美来充实孩子的精神生活，让美来帮助塑造孩子健康的心理。\\n4、在条件可能的情况下，可组织学生春游、郊游、野炊等活动，学生也可以利用寒、曙假、节假日到一些名胜古迹去游览、旅游、参观、陶冶自己的情操，走进大自然，亲近大自然，细心体会大自然，不仅能使人心胸开阔、情绪放松，精神振奋，还常能使人领悟到人生的真谛。\\n只有这样，优化了学生的心理特点，才能促使学生健康成长，从而成为新世纪的合格人才。'}\n",
      "第 4 行数据: {'text': '我们生产的食品消泡剂，具有可以快速消除泡沫的特点。\\n丹东食品消泡剂相关内容：一般而言，纯水和纯表面活性剂不起泡，这是因为它们的表面和内部是均匀的，很难形成弹性薄膜，即使形成亦不稳定，会瞬间消失。\\n丹东食品消泡剂选择：\\n1. 相容性：相容性是指两种或者两种以上物质混合时，不产生相斥分离现象的能力，相容性好，消泡剂就能够长期、稳定、均匀地存在于体系中，进而发挥消抑泡的作用；反之，就会出现分层等现象，使消泡剂的消泡工作无法正常进行。\\n2. 消泡能力：消泡能力是消泡剂的最主要性能，鉴别此项性能的标准是在同等条件下，分别加入等量不同的消泡剂，观察消泡剂的消泡速度。'}\n",
      "第 5 行数据: {'text': '程总在座谈中首先向学校的客人介绍了三一集团和北京三一重机的情况，以及在公司快速发展过程中对人才的渴求，指出通过校企联合，学校可以依靠企业的参与制定人才培养方案，使培养的人才更贴近市场，贴近企业，又可以借助企业的资源充实学校的办学实力。同时校企联合有利于企业的可持续发展。校企联合是企业实现人才战略的途径。企业在与高等职业教育合作过程中可以贯彻自己的培养意向，满足对生产第一线实用型人才的需求。\\n武汉交通职业学院盛建龙院长和河北工业职业技术学院李军锁副院长分别介绍了各自学校人才培养情况，并对三一集团的高速发展表示钦佩和赞赏，表示将和公司开展深入、全面的合作，优势互补，使学校和企业实现充分的资源共享，建立全方位长效合作机制。\\n本次联合办学签约仪式，是北京桩机高起点校企合作的开始。按照北京桩机人力资源提升计划，明年北京桩机将和所高职高专院校进行联合办学成立“三一班”，均为统招大专高技学历层次，涉及焊接、装配、机加工、售后服务等紧缺工种，“三一班”学员将达到近300人，为北京桩机的下一个五年跨越式发展打下良好的人才基础。'}\n"
     ]
    }
   ],
   "source": [
    "def preview_dataset(file_path, num_lines=5):\n",
    "    \"\"\"\n",
    "    读取并展示数据集的前 num_lines 行\n",
    "    \"\"\"\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} 文件不存在，请检查路径！\")\n",
    "\n",
    "    # 逐行读取并展示前 num_lines 行\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for idx, obj in enumerate(reader):\n",
    "            print(f\"第 {idx + 1} 行数据: {obj}\")\n",
    "            if idx + 1 >= num_lines:\n",
    "                break\n",
    "\n",
    "# 指定文件路径和需要展示的行数\n",
    "file_path = 'MateConv/dataset/mobvoi_seq_monkey_general_open_corpus.jsonl'\n",
    "preview_dataset(file_path, num_lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a655d-02e9-48f2-b7f3-164f49dfd5cd",
   "metadata": {},
   "source": [
    "- Step 4.统计与清理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075d0ca7-57ac-4b99-8421-ee58b46226fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_lines(file_path):\n",
    "    \"\"\"\n",
    "    获取 JSONL 文件的总行数，不忽略错误，保证能够全面统计。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:  # 使用二进制模式避免编码问题\n",
    "        return sum(1 for _ in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b190ec6-a982-4080-8430-2792201557d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_jsonl_format(file_path):\n",
    "    \"\"\"\n",
    "    检查 JSONL 文件中的每一行是否是有效的 JSON 格式，带进度显示，并统计所有有问题的行。\n",
    "    \"\"\"\n",
    "    total_lines = get_total_lines(file_path)  # 获取文件总行数\n",
    "    valid_lines = 0\n",
    "    invalid_lines = 0\n",
    "\n",
    "    # 使用逐行读取，捕获 JSON 和编码错误\n",
    "    with open(file_path, 'rb') as f:  # 使用二进制读取避免编码问题\n",
    "        # 使用 tqdm 进度条显示检查进度\n",
    "        for idx, line in tqdm(enumerate(f), total=total_lines, desc=\"Checking JSONL format\"):\n",
    "            try:\n",
    "                # 先尝试将每行数据解码为 UTF-8\n",
    "                decoded_line = line.decode('utf-8')\n",
    "                # 然后检查是否是有效的 JSON 格式\n",
    "                obj = jsonlines.Reader([decoded_line]).read()\n",
    "                valid_lines += 1\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"Encoding error at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "            except jsonlines.InvalidLineError as e:\n",
    "                print(f\"Invalid JSON at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "\n",
    "    print(f\"检查完成，文件中共有 {valid_lines} 行有效的 JSON 数据，{invalid_lines} 行无效的 JSON 数据。\")\n",
    "    return valid_lines, invalid_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2dd08cb-05f9-4cb6-9075-2e723f98fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking JSONL format: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 13000000/13000000 [03:37<00:00, 59635.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查完成，文件中共有 13000000 行有效的 JSON 数据，0 行无效的 JSON 数据。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_lines, invalid_lines = check_jsonl_format(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9ac12c-2098-4fa5-b8fb-415120b4d1fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/mobvoi_seq_monkey_general_open_corpus.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m                 outfile\u001b[38;5;241m.\u001b[39mwrite(line)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 使用该函数删除第 9598787 行并保存为新文件\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mremove_invalid_line\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/mobvoi_seq_monkey_general_open_corpus.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/mobvoi_seq_monkey_general_open_corpus_cleaned.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minvalid_line_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9598787\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mremove_invalid_line\u001b[0;34m(file_path, output_path, invalid_line_num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_invalid_line\u001b[39m(file_path, output_path, invalid_line_num):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    读取文件，跳过指定的无效行，并将结果写入新文件\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile, \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(infile):\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m!=\u001b[39m invalid_line_num:  \u001b[38;5;66;03m# 跳过无效行\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/mobvoi_seq_monkey_general_open_corpus.jsonl'"
     ]
    }
   ],
   "source": [
    "def remove_invalid_line(file_path, output_path, invalid_line_num):\n",
    "    \"\"\"\n",
    "    读取文件，跳过指定的无效行，并将结果写入新文件\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as infile, open(output_path, 'wb') as outfile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            if idx + 1 != invalid_line_num:  # 跳过无效行\n",
    "                outfile.write(line)\n",
    "\n",
    "# 使用该函数删除第 9598787 行并保存为新文件\n",
    "remove_invalid_line('./dataset/mobvoi_seq_monkey_general_open_corpus.jsonl',\n",
    "                    './dataset/mobvoi_seq_monkey_general_open_corpus_cleaned.jsonl', \n",
    "                    invalid_line_num=9598787)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed35dc-2071-4b83-bf55-54b5126b9c7c",
   "metadata": {},
   "source": [
    "- Step 5.定义处理函数（逐块处理数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a088ccd7-6d94-4f9e-9980-9fb2dfe5cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seq_monkey(chunk_size=50000):\n",
    "    \"\"\"\n",
    "    逐块读取 mobvoi_seq_monkey_general_open_corpus.jsonl 文件，\n",
    "    对文本进行分词，并将分词结果保存为二进制文件，支持跳过无效行，并显示处理进度。\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    chunk_idx = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    # 先计算总行数以便显示进度\n",
    "    with open('MateConv/dataset/mobvoi_seq_monkey_general_open_corpus.jsonl', 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    # 打开jsonlines文件逐行读取\n",
    "    with jsonlines.open('MateConv/dataset/mobvoi_seq_monkey_general_open_corpus.jsonl') as reader:\n",
    "        # 使用 tqdm 进度条显示进度\n",
    "        with tqdm(total=total_lines, desc=\"Processing lines\") as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    # 使用 itertools.islice 按块读取文件，每次读取 chunk_size 行数据\n",
    "                    chunk = list(itertools.islice(reader, chunk_size))\n",
    "                except jsonlines.InvalidLineError as e:\n",
    "                    print(f\"Skipping invalid chunk at chunk {chunk_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if not chunk:  # 如果读取到文件末尾，则停止\n",
    "                    break\n",
    "\n",
    "                # 遍历块中的每一行数据\n",
    "                for idx, obj in enumerate(chunk):\n",
    "                    try:\n",
    "                        # 从每一行数据中提取'text'字段（即文本内容）\n",
    "                        content = obj.get('text', '')\n",
    "                        \n",
    "                        # 跳过长度超过512的文本\n",
    "                        if len(content) > 512:\n",
    "                            continue\n",
    "\n",
    "                        # 对文本进行分词，将其转为 token ids 序列，并加上BOS和EOS标记\n",
    "                        text_id = tokenizer(f'{bos_token}{content}{eos_token}').data['input_ids']\n",
    "                        \n",
    "                        # 将分词结果添加到 doc_ids 列表中\n",
    "                        doc_ids += text_id\n",
    "\n",
    "                    except UnicodeDecodeError as e:\n",
    "                        # 如果遇到编码错误，跳过该行，并打印错误信息\n",
    "                        print(f\"Skipping invalid line {chunk_idx * chunk_size + idx + 1}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # 每处理完一块数据，更新 chunk_idx 并打印进度信息\n",
    "                chunk_idx += 1\n",
    "                pbar.update(len(chunk))  # 更新进度条\n",
    "\n",
    "                # 如果累积的 token ids 超过 1,000,000 个，保存到文件中\n",
    "                if len(doc_ids) > 1000000:\n",
    "                    arr = np.array(doc_ids, dtype=np.uint16)\n",
    "                    with open(f'MateConv/dataset/clean_seq_monkey.bin', 'ab') as f:\n",
    "                        f.write(arr.tobytes())\n",
    "                    doc_ids = []\n",
    "\n",
    "    # 如果处理完所有数据后 doc_ids 中还有未保存的内容，最后再保存一次\n",
    "    if doc_ids:\n",
    "        arr = np.array(doc_ids, dtype=np.uint16)\n",
    "        with open(f'MateConv/dataset/clean_seq_monkey.bin', 'ab') as f:\n",
    "            f.write(arr.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487db639-fa86-4df8-9801-93b3b9be613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_process():\n",
    "    \"\"\"\n",
    "    函数的作用是调用 process_seq_monkey() 函数生成数据，\n",
    "    然后整合所有生成的二进制文件，并将其合并保存为一个总的预训练数据文件。\n",
    "    \"\"\"\n",
    "    # 调用 process_seq_monkey 函数处理数据\n",
    "    process_seq_monkey()\n",
    "\n",
    "    # 数据文件路径列表，目前只处理 clean_seq_monkey.bin 文件\n",
    "    data_path_list = [\n",
    "        'MateConv/dataset/clean_seq_monkey.bin'\n",
    "    ]\n",
    "    \n",
    "    data_lst = []\n",
    "    \n",
    "    # 读取生成的二进制文件\n",
    "    for data_path in data_path_list:\n",
    "        with open(data_path, 'rb') as f:\n",
    "            # 将二进制文件中的内容加载到 numpy 数组中\n",
    "            data = np.fromfile(f, dtype=np.uint16)\n",
    "            data_lst.append(data)\n",
    "\n",
    "    # 将所有读取到的数据合并为一个大数组\n",
    "    arr = np.concatenate(data_lst)\n",
    "    print(f\"合并后的数据大小: {arr.shape}\")\n",
    "\n",
    "    # 将合并后的数据保存为最终的预训练数据文件\n",
    "    with open('MateConv/dataset/pretrain_data.bin', 'wb') as f:\n",
    "        f.write(arr.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8649ec9-46e3-4e0a-8789-d653012b8219",
   "metadata": {},
   "source": [
    "- 运行数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e7897d-e2ec-43c0-88ad-88d733dfc8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 13000000/13000000 [29:21<00:00, 7378.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的数据大小: (1510396873,)\n"
     ]
    }
   ],
   "source": [
    "pretrain_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece40cee-d5d0-4e07-b60d-394193e10724",
   "metadata": {},
   "source": [
    "- 15个epoch预训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92bb6d6-7859-452c-a326-16acc2600b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 16:04:12,904] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "[2024-11-16 16:04:14,502] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-11-16 16:04:14,502] [INFO] [runner.py:585:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain.py --epochs 15\n",
      "[2024-11-16 16:04:17,679] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-16 16:04:19,766] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.17.1-1\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:164:main] dist_world_size=4\n",
      "[2024-11-16 16:04:19,767] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-11-16 16:04:19,768] [INFO] [launch.py:256:main] process 722152 spawned with command: ['/opt/conda/bin/python', '-u', 'pretrain.py', '--local_rank=0', '--epochs', '15']\n",
      "[2024-11-16 16:04:19,770] [INFO] [launch.py:256:main] process 722153 spawned with command: ['/opt/conda/bin/python', '-u', 'pretrain.py', '--local_rank=1', '--epochs', '15']\n",
      "[2024-11-16 16:04:19,772] [INFO] [launch.py:256:main] process 722154 spawned with command: ['/opt/conda/bin/python', '-u', 'pretrain.py', '--local_rank=2', '--epochs', '15']\n",
      "[2024-11-16 16:04:19,773] [INFO] [launch.py:256:main] process 722155 spawned with command: ['/opt/conda/bin/python', '-u', 'pretrain.py', '--local_rank=3', '--epochs', '15']\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/pretrain.py\", line 16, in <module>\n",
      "    from model.dataset import PretrainDataset\n",
      "ModuleNotFoundError: No module named 'model.dataset'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/pretrain.py\", line 16, in <module>\n",
      "    from model.dataset import PretrainDataset\n",
      "ModuleNotFoundError: No module named 'model.dataset'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/pretrain.py\", line 16, in <module>\n",
      "    from model.dataset import PretrainDataset\n",
      "ModuleNotFoundError: No module named 'model.dataset'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/pretrain.py\", line 16, in <module>\n",
      "    from model.dataset import PretrainDataset\n",
      "ModuleNotFoundError: No module named 'model.dataset'\n",
      "[2024-11-16 16:04:22,777] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 722152\n",
      "[2024-11-16 16:04:22,777] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 722153\n",
      "[2024-11-16 16:04:22,787] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 722154\n",
      "[2024-11-16 16:04:22,793] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 722155\n",
      "[2024-11-16 16:04:22,796] [ERROR] [launch.py:325:sigkill_handler] ['/opt/conda/bin/python', '-u', 'pretrain.py', '--local_rank=3', '--epochs', '15'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --master_port 29500 --num_gpus=4 pretrain.py --epochs 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d1e47-8334-4eaa-a147-f352b6c47819",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from contextlib import nullcontext\n",
    "from model.model import Transformer\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import PretrainDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "\n",
    "\n",
    "def get_lr(it, all):\n",
    "    warmup_iters = args.warmup_iters\n",
    "    lr_decay_iters = all\n",
    "    min_lr = args.learning_rate / 10\n",
    "\n",
    "    if it < warmup_iters:\n",
    "        return args.learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def train_epoch(epoch, wandb):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y) in enumerate(train_loader):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            out = model(X, Y)\n",
    "            loss = out.last_loss / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n",
    "                    epoch,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss.item() * args.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/pretrain_{lm_config.dim}{moe_path}.pth'\n",
    "\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    model = Transformer(lm_config).to(args.device)\n",
    "    moe_path = '_moe' if lm_config.use_moe else ''\n",
    "\n",
    "    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "\n",
    "\n",
    "# torchrun --nproc_per_node 2 1-pretrain.py\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MiniMind Pretraining\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"out\", help=\"Output directory\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", help=\"Device to use\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"Data type\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-Pretrain\", help=\"Weights & Biases project name\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8, help=\"Number of workers for data loading\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./dataset/pretrain_data.bin\", help=\"Path to training data\")\n",
    "    parser.add_argument(\"--ddp\", action=\"store_true\", help=\"Use DistributedDataParallel\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=8, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"Gradient clipping threshold\")\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"Number of warmup iterations\")\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"Logging interval\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"Model saving interval\")\n",
    "\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    lm_config = LMConfig()\n",
    "    max_seq_len = lm_config.max_seq_len\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    tokens_per_iter = args.batch_size * max_seq_len\n",
    "    torch.manual_seed(1337)\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    args.wandb_run_name = f\"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"\n",
    "\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    ddp_local_rank, DEVICE = 0, \"cuda:0\"\n",
    "    if ddp:\n",
    "        init_distributed_mode()\n",
    "        args.device = torch.device(DEVICE)\n",
    "\n",
    "    if args.use_wandb and (not ddp or ddp_local_rank == 0):\n",
    "        import wandb\n",
    "        wandb.init(project=args.wandb_project, name=args.wandb_run_name)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    data_path_list = [args.data_path]\n",
    "    train_ds = PretrainDataset(data_path_list, max_length=max_seq_len, memmap=True)\n",
    "    train_sampler = DistributedSampler(train_ds) if ddp else None\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    model = init_model()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    if False and platform.system() != 'Windows' and float(torch.__version__.split('.')[0]) >= 2:\n",
    "        Logger(\"compiling the model... (takes a ~minute)\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    if ddp:\n",
    "        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}\n",
    "        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "    iter_per_epoch = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c203a2-b0d9-40f6-b07b-803db3ba875b",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649752b-e5ca-42ff-81ec-bf0afd0b0762",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/0d46acea891701fd573115782288e05.jpg\" alt=\"0d46acea891701fd573115782288e05\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d524605-a791-4704-9626-62ea18926667",
   "metadata": {},
   "source": [
    "训练结束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3114ad-174c-4fce-8fbc-4abd657f19ab",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/9f1b2321dc79550b1e0f194e165ff22.png\" alt=\"9f1b2321dc79550b1e0f194e165ff22\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184bee3-759b-4bdd-87f4-de51abb14e69",
   "metadata": {},
   "source": [
    "测试运行（单字符预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eaf2280-983a-4fb8-9b34-27c895b1c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的模块\n",
    "import torch\n",
    "from model.model import Transformer  # 确保路径正确\n",
    "from model.LMConfig import LMConfig   # 导入 LMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d917b1-dd6a-4211-bcab-3026ae665e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建配置对象\n",
    "lm_config = LMConfig(\n",
    "    dim=512,                # 模型的维度\n",
    "    n_layers=8,            # 层数\n",
    "    n_heads=16,            # 注意力头数\n",
    "    vocab_size=6400,       # 词汇表大小\n",
    "    max_seq_len=512,       # 最大序列长度\n",
    "    dropout=0.1            # Dropout 概率\n",
    "    # 这里可以添加更多配置，根据需要进行调整\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5993ce-8479-4c73-abe7-69c7cad07c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Transformer 模型\n",
    "model = Transformer(lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff44e2ab-2b33-4d3a-8f84-082d84f74580",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a22c151-5f07-44c7-a44a-bdb9de35a1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487f2dc2-4469-4abb-8503-e9613bb594ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(6400, 512)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# 检查模型结构和参数\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f14b56c-3699-42c7-9a93-54a6db485ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(6400, 512)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load('out/pretrain_512.pth', map_location=device))\n",
    "model.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1d28830-f20c-4ddd-b992-3ccd2ae14be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入文本\n",
    "input_text = \"你好，我是\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e6e7069-e23b-4400-ad59-d69a7ea30359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "个\n"
     ]
    }
   ],
   "source": [
    "# 生成输出\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    generated_ids = output.logits.argmax(dim=-1)  # 假设使用 argmax 选择输出\n",
    "    generated_text = tokenizer.decode(generated_ids[0])\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd37a0-ef5f-4028-9530-ea00bad69c3b",
   "metadata": {},
   "source": [
    "测试运行（多字符预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef773698-ca74-4d69-84df-4d9d88b6d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入文本\n",
    "input_text = \"长江、\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7c78358-e3bb-4884-bbdf-1dc263018637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长江、淮河、汉江、汉江、汉江、大渡河、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉江、汉\n"
     ]
    }
   ],
   "source": [
    "# 生成多个 token\n",
    "num_tokens_to_generate = 100  # 要生成的 token 数量\n",
    "generated_tokens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        output = model(input_ids)\n",
    "        next_token = output.logits.argmax(dim=-1)[:, -1]  # 获取最后一个 token 的预测\n",
    "        generated_tokens.append(next_token.item())  # 将 token ID 添加到列表中\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)  # 将新 token 添加到输入中\n",
    "\n",
    "# 将生成的 token IDs 转换为文本\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# 打印最终回复\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5279f206-d115-4695-9746-764ee0d1b725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，但他在2019年的年度最佳新秀中，以4-0的成绩夺得了最佳新秀奖。\n",
      "在2019年的年度最佳新秀中，他以4-0的成绩夺得了最佳新秀奖。在2019年的年度最佳新秀中，他以4-0的成绩夺得了最佳新秀奖。\n",
      "在2019年的年度最佳新\n"
     ]
    }
   ],
   "source": [
    "# 准备输入文本\n",
    "input_text = \"美国人\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# 生成多个 token\n",
    "num_tokens_to_generate = 100  # 要生成的 token 数量\n",
    "generated_tokens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        output = model(input_ids)\n",
    "        next_token = output.logits.argmax(dim=-1)[:, -1]  # 获取最后一个 token 的预测\n",
    "        generated_tokens.append(next_token.item())  # 将 token ID 添加到列表中\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)  # 将新 token 添加到输入中\n",
    "\n",
    "# 将生成的 token IDs 转换为文本\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# 打印最终回复\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8990bbc-7625-4c5e-8020-3c4e8890518b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cec9b-2faf-4fe3-a74d-f1046c16088a",
   "metadata": {},
   "source": [
    "- 备用代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9518c4a6-bd36-41e0-b94a-e878b64bb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seq_monkey(chunk_size=50000):\n",
    "    \"\"\"\n",
    "    逐块读取 mobvoi_seq_monkey_general_open_corpus.jsonl 文件，\n",
    "    对文本进行分词，并将结果保存为二进制文件。\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    chunk_idx = 0\n",
    "\n",
    "    # 打开jsonlines文件逐行读取\n",
    "    with jsonlines.open('./dataset/mobvoi_seq_monkey_general_open_corpus.jsonl') as reader:\n",
    "        # 循环读取文件的每个块\n",
    "        while True:\n",
    "            # 使用 itertools.islice 按块读取文件，每次读取 chunk_size 行数据\n",
    "            chunk = list(itertools.islice(reader, chunk_size))\n",
    "            if not chunk:  # 如果读取到文件末尾，则停止\n",
    "                break\n",
    "\n",
    "            # 遍历块中的每一行数据\n",
    "            for idx, obj in enumerate(chunk):\n",
    "                try:\n",
    "                    # 从每一行数据中提取'text'字段\n",
    "                    content = obj.get('text', '')\n",
    "\n",
    "                    # 跳过长度超过512的文本\n",
    "                    if len(content) > 512:\n",
    "                        continue\n",
    "\n",
    "                    # 对文本进行分词，将其转为 token ids 序列，并加上BOS和EOS标记\n",
    "                    text_id = tokenizer(f'{bos_token}{content}{eos_token}').data['input_ids']\n",
    "                    doc_ids += text_id\n",
    "                \n",
    "                except UnicodeDecodeError as e:\n",
    "                    # 打印错误详细信息和导致问题的文本\n",
    "                    print(f\"UnicodeDecodeError on chunk {chunk_idx + 1}, line {idx + 1}: {e}\")\n",
    "                    print(f\"Problematic content: {content}\")\n",
    "                    continue  # 跳过有问题的行，继续处理\n",
    "\n",
    "            # 每处理完一块数据，更新 chunk_idx 并打印进度信息\n",
    "            chunk_idx += 1\n",
    "            print(f\"Processed chunk {chunk_idx} with {chunk_size} lines\")\n",
    "\n",
    "            # 如果累积的 token ids 超过 1,000,000 个，保存到文件中\n",
    "            if len(doc_ids) > 1000000:\n",
    "                arr = np.array(doc_ids, dtype=np.uint16)\n",
    "                with open(f'./dataset/clean_seq_monkey.bin', 'ab') as f:\n",
    "                    f.write(arr.tobytes())\n",
    "                doc_ids = []\n",
    "\n",
    "    # 如果处理完所有数据后 doc_ids 中还有未保存的内容，最后再保存一次\n",
    "    if doc_ids:\n",
    "        arr = np.array(doc_ids, dtype=np.uint16)\n",
    "        with open(f'./dataset/clean_seq_monkey.bin', 'ab') as f:\n",
    "            f.write(arr.tobytes())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
